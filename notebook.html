<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
        <link rel="stylesheet" type="text/css" href="style.css" />
        <title>Xu-Sheng Li (Ph.D student at Chongqing University)</title>
    </head>
    <body>
        <h1 style="padding-left: 0.5em">Xu-Sheng Li (Ph.D student at Chongqing University)</h1>
        <table summary="Table for page layout." id="tlayout">
        <tr  valign="top">
            <td id="layout-menu">
                <div class = "menu-item"> <a href="index.html"> Home </a></div>
                <div class="menu-item"><a href="publication.html" class="current">Publications</a></div>
                <div class="menu-item"><a href="notebook.html" class="current">Notebook</a></div>
            </td>
            <td id="layout-content">
                <h1 style="margin-top: 0em">Notebook</a></h1><br>
            <div>
                    <h2><hr><a name="conference"></a>Read List</h2>
                    <ol>
                        <li><p>
                            IoU<br>
                            Rethinking IoU-based Optimization for Single-stage 3D Object Detection<br>
                          
                        </p></li>
                        <li><p>
                            BEV<br>
                            Lift, Splat, Shoot: Encoding Images from Arbitrary Camera Rigs by Implicitly Unprojecting to 3D<br>
                            BEVDet: High-Performance Multi-Camera 3D Object Detection in Bird-Eye-View<br>
                            M2BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation<br>
                            BEVDet4D: Exploit Temporal Cues in Multi-camera 3D Object Detection<br>
                            BEVerse: Unified Perception and Prediction in Birds-Eye-View for Vision-Centric Autonomous Driving<br>
                            BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird' s-Eye View Representation<br>
                            BEVDepth: Acquisition of Reliable Depth for Multi-view 3D Object Detection<br>
                            DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries<br>
                            PETR: Position Embedding Transformation for Multi-View 3D Object Detection<br>
                        </p></li>
                    </ol>
                </div>
                <div>
                    <h2><hr><a name="conference"></a>AAAI 2022</h2>
                    <ol>
                        <li><p>
                            Point cloud Attack<br>
                            Shape Prior Guided Attack: Sparser Perturbations on 3D Point Clouds<br>
                            FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack<br>
                            [<A HREF="url">PDF</A>][<A HREF="url">CODE</A>]
                        </p></li>
                        <li><p>
                            3D Object Detection<br>
                            SCIR-Net: Structured Color Image Representation Based 3D Object Detection
Network from Point Clouds<br>
Behind the Curtain: Learning Occluded Shapes for 3D Object Detection<br>
SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds<br>
JPV-Net: Joint Point-Voxel Representations for Accurate 3D Object Detection<br>
SASA: Semantics-Augmented Set Abstraction for Point-Based 3D Object Detection<br>
                            [<A HREF="url">PDF</A>]
                        </p></li>
                        <li><p>
                            Monocular 3D<br>
                            Learning Auxiliary Monocular Contexts Helps Monocular 3D Object Detection<br>
                        </p></li>
                    </ol>
                </div>




<div>
                    <h2><hr><a name="conference"></a>实验记录</h2>
                    <ol>
                        <li><p>
                            (OpenPCDet_mamba) lj123@1413e92b32d9:~/OpenPCDet_mamba/tools$ python train.py --cfg_file cfgs/waymo_models/voxelnext2d_ioubranch-voxel2pillar.yaml<br>
                            验证新的编码方法和voxel to pillar。
                        </p></li>
                    </ol>
                </div>


                

                <div>
                    <h2><hr><a name="conference"></a>CVPR 2022</h2>
                    <ol>
                        <li><p>
                            Transformer<br>
                            Understanding The Robustness in Vision Transformers<br>
                            [<A HREF="url">PDF</A>][<A HREF="url">CODE</A>]
                        </p></li>
                        <li><p>
                            Integration<br>
                            On the Integration of Self-Attention and Convolution<br>
                            [<A HREF="url">PDF</A>][<A HREF="url">CODE</A>]
                        </p></li>
                    </ol>
                </div>
                <div>
                    <h2><hr><a name="conference"></a>TORCHSPARSE</h2>
                    <ol>
                        <li><p>
                             Transformer<br>
                            TORCHSPARSE: EFFICIENT POINT CLOUD INFERENCE ENGINE<br>
                            Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution<br>
                            [<A HREF="url">PDF</A>][<A HREF="url">CODE</A>]
                        </p></li>
                    </ol>
                </div>
            </td>
        </tr>
    </table>
    </body>
</html>
